%% ------------------------------------------------------------------------- %%
\chapter{Introduction}
\label{cap:introduction}

%% ------------------------------------------------------------------------- %%
\section{Motivation}
\label{sec:motivation}

Machine Learning techniques are widely used in the modern industry to leverage useful insights and applications from data. In a range of very different areas, from medical labs to financial institutions, machine learning models are already a fundamental part of the business. There is a rising trend of academic research on the theory and applications of machine learning techniques, which is evidence of the growing importance of robust machine learning models for modern scientific and industrial applications.

Gradient Boosting Machines is one of said techniques which has been achieving state-of-the-art results, especially with problems in structured datasets. \textit{State of Data Science and Machine Learning 2019} from \cite{kaggle:survey}, an industry-wide survey that presents a comprehensive view of the state of data science and machine learning, reports that GBMs is the third most used algorithm chosen by the respondents. The XGBoost and LightGBM libraries are the commonly used implementations nowadays, using decision trees as the base learners in the algorithm.

In the applied research and industry a considerable amount of a data scientist's time is reserved to experimentation and improvements to new or existing machine learning models. An important component of this process is the experimentation with parameters which the algorithm does not learn directly from the data, the \textit{hyperparameters}. Despite the increase in development of automated tuning techniques and the current available hyperparameter optimization methods, XGBoost and LightGBM libraries have several hyperparameters to be optimized, which depending on the amount of data and desired number of hyperparameters renders these tuning techniques infeasible.

Recent research has been tackling this problem with different solutions and perspectives. In \cite{probst2018tunability}, the concept of hyperparameter \textit{tunability} is formalized and new default values of hyperparameters are calculated for some algorithms. In another paper, \cite{van2018hyperparameter} attemps to measure the relative importance of each hyperparameter over different datasets, including the gradient boosting algorithm AdaBoost.

To expand on the knowledge of how hyperparameters alter the machine learning model performance, this study proposes a large-scale experiment relating dataset's characteristics, gradient boosting classification models, a set of hyperparameters and selected evaluation metrics for classification tasks.

%% ------------------------------------------------------------------------- %%
\section{Objective}
\label{sec:objetivo}

The main objective of this work is to study and obtain insights about the main hyperparameters of gradient boosting binary classification models, using a statistically robust framework in the analysis. The research objective of the results can be summarized in the following viewpoints:

\begin{enumerate}
  \item How each type of hyperparameter (or combinations of them) affects the performance of the model, overall. Is there a hyperparameter which does not affect much the performance? What is the combination of hyperparameter that impacts the most? How sensitive is the gradient boosting algorithm to the studied hyperparameters?
  \item How different characteristics of a dataset affect the hyperparameters impact, if there is a difference in them. Datasets with high number of features will have a different impact on hyperparameter? How the datasets are sensitive to hyperparameter changes in the algorithm?
  \item How the performance metrics of classification models relate, obtain insights about the relative behavior of the performance metrics and the hyperparameters. Is there a pattern in the metrics behavior? Which combination of hyperparameter causes the highest change in a performance metric?
\end{enumerate}


%% ------------------------------------------------------------------------- %%
\section{Structure}
\label{sec:organizacao_trabalho}

The next two chapters of this work, Chapter \ref{cap:ml-fundamentals} and Chapter \ref{cap:boosting-intro} introduce the required theoretical background for supervised machine learning and gradient boosting machines, including an explanation of the studied hyperparameters. Chapter \ref{cap:study-methodology} describe the structure of the experiment, which datasets were used, how the hyperparameters values were created and the experiment pipeline. In Chapter \ref{cap:experimental-analysis} the experimental analysis is outlined, with the description of the aggregation method used, definitions of concepts for the final analysis and a complete characterization of the statistical framework used in the results. The main results are presented in \ref{cap:results}, with the outcomes observed from the analysis along with discussions of what was observed. Finally, Chapter \ref{cap:conclusion} present the conclusions from the overall study and a discussion about some of its limitations that could be addressed in future work.